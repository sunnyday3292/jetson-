# -*- coding: utf-8 -*-
"""
Jetsonìš© DQN Lane-keeping / Obstacle Avoidance
ìˆœì°¨ í•™ìŠµ + (ì„ íƒ)EWC + ë°ì´í„° ìºì‹± + 10íšŒ ë°˜ë³µ ì‹¤í—˜ ë²„ì „
+ Continual Learning ì§€í‘œ: FWT, BWT, IM (ê° ì‘ì—… ì¢…ë£Œ ì‹œë§ˆë‹¤ ê³„ì‚°)


ì£¼ì˜:
- ê²½ê³  í•´ê²°: torch.tensor(list-of-ndarrays) ê¸ˆì§€ â†’ np.stack/np.array í›„ torch.from_numpy ì‚¬ìš©
- ê²½ê³  í•´ê²°: np.bool_ ì¸ë±ì‹± ê¸ˆì§€ â†’ doneì„ float32(0.0/1.0)ë¡œ ì‚¬ìš©
"""
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random
import os
import matplotlib.pyplot as plt
import time


# -----------------------------
# Device
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)
if torch.cuda.is_available():
   print(f"CUDA Device: {torch.cuda.get_device_name(0)}")


# -----------------------------
# ëœë¤ ì‹œë“œ ì„¤ì • (ì‹œê°„ ê¸°ë°˜, ì¬í˜„ ê°€ëŠ¥í•˜ë©´ì„œë„ ì‹¤í–‰ë§ˆë‹¤ ë‹¤ë¦„)
# -----------------------------
def set_seed_with_time():
   import time
   seed = int(time.time() * 1000) % 1000000  # í˜„ì¬ ì‹œê°„(ë°€ë¦¬ì´ˆ) ê¸°ë°˜
   random.seed(seed)
   np.random.seed(seed)
   torch.manual_seed(seed)
   if torch.cuda.is_available():
       torch.cuda.manual_seed(seed)
       torch.cuda.manual_seed_all(seed)
   print(f"Random seed set to: {seed}")  # ë””ë²„ê¹…ìš©


# -----------------------------
# LaneDetector
# -----------------------------
class LaneDetector:
   def __init__(self):
       self.prev_lane = 1  # ì•ˆì „í•œ ì´ˆê¸°ê°’(ì˜¤ë¥¸ìª½)
       # ë¹¨ê°• HSV ë²”ìœ„
       self.lower_red1 = np.array([0, 70, 50])
       self.upper_red1 = np.array([10, 255, 255])
       self.lower_red2 = np.array([170, 70, 50])
       self.upper_red2 = np.array([180, 255, 255])


   def process_frame(self, frame):
       height, width = frame.shape[:2]
       hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)


       # 1. ë…¸ë€ìƒ‰ ë§ˆìŠ¤í¬ ìƒì„±
       mask_yellow = cv2.inRange(
           hsv, np.array([15, 80, 100]), np.array([35, 255, 255])
       )


       # 2. í°ìƒ‰ ë§ˆìŠ¤í¬ ìƒì„± (ì‹œê°í™”ìš©)
       lower_white = np.array([0, 0, 180])
       upper_white = np.array([255, 30, 255])
       mask_white = cv2.inRange(hsv, lower_white, upper_white)


       # 3. ë…¸ë€ìƒ‰ê³¼ í°ìƒ‰ ë§ˆìŠ¤í¬ë¥¼ í•©ì¹¨ (ì‹œê°í™”ìš©)
       lane_mask = cv2.bitwise_or(mask_yellow, mask_white)  # noqa: F841 (ì‹œê°í™”ì‹œ ì‚¬ìš© ê°€ëŠ¥)


       # ì°¨ì„  ë°©í–¥ íŒë‹¨ì€ 'ë…¸ë€ìƒ‰' ë§ˆìŠ¤í¬ ê¸°ì¤€
       left_half = mask_yellow[:, : width // 2]
       right_half = mask_yellow[:, width // 2 :]
       left_count = cv2.countNonZero(left_half)
       right_count = cv2.countNonZero(right_half)


       if left_count > right_count and left_count > 0:
           lane_state = 1  # "right"
       elif right_count > left_count and right_count > 0:
           lane_state = 0  # "left"
       else:
           lane_state = self.prev_lane


       self.prev_lane = lane_state


       # ë¹¨ê°„ìƒ‰ ê°ì§€
       mask_red1 = cv2.inRange(hsv, self.lower_red1, self.upper_red1)
       mask_red2 = cv2.inRange(hsv, self.lower_red2, self.upper_red2)
       mask_red = cv2.bitwise_or(mask_red1, mask_red2)
       red_pixels = cv2.countNonZero(mask_red)
       red_ratio = red_pixels / float(height * width)


       # --- ### 1. ë…¸ë€ìƒ‰ ì°¨ì„ ì˜ ë¬´ê²Œ ì¤‘ì‹¬ ê³„ì‚° ### ---
       yellow_center_x = -1
       M_yellow = cv2.moments(mask_yellow)
       if M_yellow["m00"] > 0:
           yellow_center_x = int(M_yellow["m10"] / M_yellow["m00"])


       # ë¹¨ê°„ìƒ‰ ê°ì²´ ì¤‘ì‹¬ ê³„ì‚°
       red_center_x = -1
       red_center_y = -1
       if red_pixels > 0:
           M_red = cv2.moments(mask_red)
           if M_red["m00"] > 0:
               red_center_x = int(M_red["m10"] / M_red["m00"])
               red_center_y = int(M_red["m01"] / M_red["m00"])


       # --- ### 2. ë…¸ë€ìƒ‰ ì¤‘ì‹¬ì„ ê¸°ì¤€ìœ¼ë¡œ ë¹¨ê°„ìƒ‰ ìœ„ì¹˜ íŒë‹¨ ### ---
       red_side_relative_to_yellow = -1
       if red_pixels / (width * height) > 0.001 and yellow_center_x != -1:
           if red_center_x < yellow_center_x:
               red_side_relative_to_yellow = 0  # "left"
           else:
               red_side_relative_to_yellow = 1  # "right"


       # ë°˜í™˜ê°’
       return lane_state, red_ratio, red_side_relative_to_yellow




# -----------------------------
# OfflineDataCollector
# -----------------------------
class OfflineDataCollector:
   def __init__(self, lane_detector):
       self.lane_detector = lane_detector
       self.before_act = None


   def _get_state(self, frame):
       lane_state, red_ratio, red_side = self.lane_detector.process_frame(frame)


       height, width, _ = frame.shape
       bottom_center_pixel = frame[height - 1, width // 2]
       over_line = float(np.all(bottom_center_pixel > 240))


       # ìƒíƒœ: lane_state ì œê±°(ëˆ„ìˆ˜ ë°©ì§€)
       state = np.array(
           [over_line, red_side, red_ratio], dtype=np.float32  # 0  # 1  # 2
       )


       return state, lane_state  # lane_stateëŠ” ë³„ë„ ë°˜í™˜


   def _calculate_reward(
       self, state, current_lane_state, next_state, next_lane_state, done=False
   ):
       over = float(state[0])
       red_side = int(state[1])
       red_ratio = float(state[2])


       if next_state is None:
           return 0.0, True


       next_red_ratio = float(next_state[2])
       r = 3.0


       # ì¥ì• ë¬¼ì´ 'í˜„ì¬ ì°¨ì„ 'ì— ìˆì„ ë•Œ íšŒí”¼ ëª»í•˜ë©´ í˜ë„í‹°
       if red_ratio > 0.06:
           if red_side == current_lane_state:
               if next_lane_state != current_lane_state:
                   r += 0.3  # íšŒí”¼ ì„±ê³µ ë³´ìƒ
               else:
                   r -= 0.5 * (red_ratio - 0.06) / 0.14  # ì‹¤íŒ¨ íŒ¨ë„í‹°


       # ì°¨ì„  ì´íƒˆ íŒ¨ë„í‹°
       if over > 0.5:
           r -= 1.0
           done = True


       # ì¥ì• ë¬¼ ë¹„ìœ¨ ê°ì†Œë¥¼ ì•½í•˜ê²Œ ì¥ë ¤
       if next_red_ratio < red_ratio:
           r += 0.01


       r = float(np.clip(r, -1.0, 1.0))
       return r, done


   def collect_from_frames(self, frames):
       state_list, action_list, reward_list, next_state_list, done_list = (
           [],
           [],
           [],
           [],
           [],
       )
       # ë§ˆì§€ë§‰ í”„ë ˆì„ì€ nextê°€ ì—†ìœ¼ë‹ˆ len-1ê¹Œì§€ë§Œ
       for idx in range(len(frames) - 1):
           frame = frames[idx]
           next_frame = frames[idx + 1]
           done = False


           state, current_lane_state = self._get_state(frame)
           next_state, next_lane_state = self._get_state(next_frame)


           # ì •ë‹µ ì•¡ì…˜: ë‹¤ìŒ í”„ë ˆì„ì˜ lane_state (ì˜ˆì¸¡ íƒ€ê¹ƒ)
           action = next_lane_state


           reward, done = self._calculate_reward(
               state, current_lane_state, next_state, next_lane_state, done=done
           )


           if state[0] > 0.5 or state[2] > 0.20:  # over_line or red_ratio
               done = True


           state_list.append(state)
           action_list.append(action)
           reward_list.append(reward)
           next_state_list.append(next_state)
           done_list.append(done)


       return state_list, action_list, reward_list, next_state_list, done_list




# -----------------------------
# DQN ëª¨ë¸
# -----------------------------
class DQN(nn.Module):
   def __init__(self, state_dim, action_dim):
       super().__init__()
       self.fc1 = nn.Linear(state_dim, 256)
       self.ln1 = nn.LayerNorm(256)
       self.fc2 = nn.Linear(256, 256)
       self.ln2 = nn.LayerNorm(256)
       self.fc3 = nn.Linear(256, action_dim)  # ê° actionë³„ Qê°’ ì¶œë ¥


   def forward(self, state):
       x = F.relu(self.ln1(self.fc1(state)))
       x = F.relu(self.ln2(self.fc2(x)))
       return self.fc3(x)  # [batch, action_dim]




# -----------------------------
# DQNAgent with optional EWC
# -----------------------------
class DQNAgent:
   def __init__(
       self,
       state_dim,
       action_dim=2,
       device="cpu",
       cql_alpha=30,
       lr=1e-5,
       gamma=0.99,
       batch_size=32,
       target_update_freq=10,
       ewc_lambda=10,
   ):
       self.device = device
       self.state_dim = state_dim
       self.action_dim = action_dim
       self.gamma = gamma
       self.cql_alpha = cql_alpha
       self.batch_size = batch_size
       self.target_update_freq = target_update_freq
       self.ewc_lambda = ewc_lambda


       self.policy_net = DQN(state_dim, action_dim).to(device)
       self.target_net = DQN(state_dim, action_dim).to(device)
       self.target_net.load_state_dict(self.policy_net.state_dict())
       self.target_net.eval()


       self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)
       self.replay_buffer = deque(maxlen=20000)


       self.task_params = {}
       self.task_fisher = {}


   def calculate_fisher_information(self, states, actions):
       self.policy_net.eval()
       self.optimizer.zero_grad()
       q_values = self.policy_net(states)
       selected_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
       # Fisher ê·¼ì‚¬: ì„ íƒ Qì˜ í‰ê· ì„ ìµœëŒ€í™”í•˜ëŠ” ê·¸ë¼ë“œì˜ ì œê³±
       loss = selected_q_values.mean()
       loss.backward()
       fisher_information = {}
       for name, param in self.policy_net.named_parameters():
           if param.grad is not None:
               fisher_information[name] = param.grad.data.clone() ** 2
       return fisher_information


   def save_params_and_fisher(self, dataset, task_name, sample_max=1024):
       print(f"[EWC] Saving parameters and Fisher for {task_name}...")
       # dataset: list of tuples (s,a,r,ns,d) â€” ì—¬ê¸°ì„œ s,aë§Œ ìƒ˜í”Œ
       if len(dataset) == 0:
           return
       # ìƒ˜í”Œ ë‹¤ìš´ìƒ˜í”Œë§
       if len(dataset) > sample_max:
           dataset = random.sample(dataset, sample_max)


       # âœ… ê²½ê³  ì—†ëŠ” í…ì„œ ìƒì„± (np.stack/np.array â†’ from_numpy)
       states_np = np.stack([x[0] for x in dataset], axis=0).astype(np.float32)
       actions_np = np.array([x[1] for x in dataset], dtype=np.int64)


       states_sample = torch.from_numpy(states_np).to(self.device)
       actions_sample = torch.from_numpy(actions_np).to(self.device)


       self.task_params[task_name] = {
           name: p.clone().detach() for name, p in self.policy_net.named_parameters()
       }
       self.task_fisher[task_name] = self.calculate_fisher_information(
           states_sample, actions_sample
       )


   def ewc_loss(self):
       if (
           (self.ewc_lambda is None)
           or (self.ewc_lambda <= 0)
           or (len(self.task_params) == 0)
       ):
           return torch.tensor(0.0, device=self.device)
       ewc_loss_val = torch.tensor(0.0, device=self.device)
       for task_name in self.task_params:
           for name, param in self.policy_net.named_parameters():
               if name in self.task_params[task_name]:
                   saved_param = self.task_params[task_name][name]
                   fisher = self.task_fisher[task_name].get(name, None)
                   if fisher is not None:
                       ewc_loss_val = (
                           ewc_loss_val + (fisher * (param - saved_param) ** 2).sum()
                       )
       return self.ewc_lambda * ewc_loss_val


   def train_offline(
       self,
       state_list,
       action_list,
       reward_list,
       next_state_list,
       done_list,
       epochs=20,
       beta=5,
       keep_buffer=False,
   ):
       if not keep_buffer:
           self.replay_buffer.clear()


       for s, a, r, ns, d in zip(
           state_list, action_list, reward_list, next_state_list, done_list
       ):
           self.replay_buffer.append((s, a, r, ns, d))


       if not self.replay_buffer:
           print("Replay buffer is empty. Skipping training.")
           return


       step_count = 0
       for epoch in range(epochs):
           batch = random.sample(
               self.replay_buffer, min(len(self.replay_buffer), self.batch_size)
           )


           # âœ… ê²½ê³  ì—†ëŠ” í…ì„œ ìƒì„± (ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
           states_np = np.stack([b[0] for b in batch], axis=0).astype(np.float32)
           actions_np = np.array([b[1] for b in batch], dtype=np.int64)
           rewards_np = np.array([b[2] for b in batch], dtype=np.float32)
           next_states_np = np.stack([b[3] for b in batch], axis=0).astype(np.float32)
           # np.bool_ ê²½ê³  íšŒí”¼: íŒŒì´ì¬ bool â†’ float32(0.0/1.0)
           dones_np = np.array([bool(b[4]) for b in batch], dtype=np.float32)


           states = torch.from_numpy(states_np).to(self.device)
           actions = torch.from_numpy(actions_np).to(self.device)
           rewards = torch.from_numpy(rewards_np).to(self.device)
           next_states = torch.from_numpy(next_states_np).to(self.device)
           dones = torch.from_numpy(dones_np).to(self.device)  # float 0/1


           with torch.no_grad():
               next_actions = self.policy_net(next_states).argmax(dim=1, keepdim=True)
               next_q_values = (
                   self.target_net(next_states).gather(1, next_actions).squeeze(1)
               )
               target_q = rewards + self.gamma * (1.0 - dones) * next_q_values


           all_q = self.policy_net(states)
           current_q = all_q.gather(1, actions.unsqueeze(1)).squeeze(1)


           logsumexp_q = torch.logsumexp(all_q, dim=1)
           cql_penalty = (logsumexp_q - current_q).mean()


           entropy_penalty = all_q.var(dim=1).mean()
           td_loss = F.mse_loss(current_q, target_q)


           loss = (
               td_loss
               + self.cql_alpha * cql_penalty
               + beta * entropy_penalty
               + self.ewc_loss()
           )


           self.optimizer.zero_grad()
           loss.backward()
           nn.utils.clip_grad_norm_(self.policy_net.parameters(), 5.0)
           self.optimizer.step()


           step_count += 1
           if step_count % self.target_update_freq == 0:
               self.target_net.load_state_dict(self.policy_net.state_dict())


           if epoch % 10 == 0:
               print(f"[Offline] Epoch {epoch:03d}, Loss: {loss.item():.4f}")




# -----------------------------
# í—¬í¼ í•¨ìˆ˜: ë¹„ë””ì˜¤/ë°ì´í„° ë¡œë”©
# -----------------------------
def load_video_frames(video_path, max_frames=None):
   if not os.path.exists(video_path):
       print(f"File not found: {video_path}")
       return []
   cap = cv2.VideoCapture(video_path)
   frames = []
   frame_count = 0
   while True:
       ret, frame = cap.read()
       if not ret:
           break
       frames.append(frame)
       frame_count += 1
       if max_frames and frame_count >= max_frames:
           break
   cap.release()
   return frames




def load_and_process_data(video_files):
   state_list, action_list, reward_list, next_state_list, done_list = (
       [],
       [],
       [],
       [],
       [],
   )
   for vf in video_files:
       print(f"Processing video: {vf}")
       frames = load_video_frames(vf)
       if not frames:
           continue
       # ë¹„ë””ì˜¤ë§ˆë‹¤ detector/collector ì´ˆê¸°í™” (prev_lane ëˆ„ì  ë°©ì§€)
       lane_detector = LaneDetector()
       collector = OfflineDataCollector(lane_detector)
       s, a, r, ns, d = collector.collect_from_frames(frames)
       state_list.extend(s)
       action_list.extend(a)
       reward_list.extend(r)
       next_state_list.extend(ns)
       done_list.extend(d)
   print(f"Generated {len(state_list)} transitions.")
   return state_list, action_list, reward_list, next_state_list, done_list




# -----------------------------
# í‰ê°€ ìœ í‹¸
# -----------------------------
def evaluate_agent(agent, eval_video_file, run_index=0):
   print(f"\n--- [Run {run_index+1}] Evaluation (test.mov, accuracy only) ---")
   eval_frames = load_video_frames(eval_video_file)
   return evaluate_on_frames(agent, eval_frames, device)




def evaluate_on_frames(agent, frames, device):
   if not frames or len(frames) < 2:
       print("Could not load evaluation frames or not enough frames to evaluate.")
       return 0.0, 0.0
   collector_eval = OfflineDataCollector(LaneDetector())
   total_reward = 0.0
   agent_actions = []
   offline_actions = []


   for i in range(len(frames) - 1):
       frame = frames[i]
       next_frame = frames[i + 1]


       state, current_lane_state = collector_eval._get_state(frame)
       next_state, next_lane_state = collector_eval._get_state(next_frame)


       correct_action = next_lane_state
       offline_actions.append(correct_action)


       reward, _ = collector_eval._calculate_reward(
           state, current_lane_state, next_state, next_lane_state
       )
       total_reward += reward


       with torch.no_grad():
           s_tensor = torch.from_numpy(
               np.asarray(state, dtype=np.float32)
           ).unsqueeze(0).to(device)
           q_values = agent.policy_net(s_tensor)
           action = q_values.argmax().item()
           agent_actions.append(action)


   offline_actions = np.array(offline_actions)
   agent_actions = np.array(agent_actions)
   acc = (
       float((offline_actions == agent_actions).mean())
       if len(offline_actions) > 0
       else 0.0
   )
   return acc, float(total_reward)




def frames_for_task(video_list, max_frames=None):
   if not video_list:
       return []
   return load_video_frames(video_list[0], max_frames=max_frames)




# -----------------------------
# ìƒˆ í—¬í¼: ëª¨ë“  ì‘ì—… í‰ê°€ + ë¶€ë¶„ ì§€í‘œ ê³„ì‚°
# -----------------------------
def evaluate_all_tasks(agent, train_order, task_frames_map, device):
   """
   í˜„ì¬ ì—ì´ì „íŠ¸ë¡œ ê° ì‘ì—… í”„ë ˆì„ì—ì„œ ì •í™•ë„ í‰ê°€ -> Rì˜ í•œ í–‰ì„ ë°˜í™˜
   """
   row = np.zeros(len(train_order), dtype=np.float32)
   for j, tj in enumerate(train_order):
       acc, _ = evaluate_on_frames(agent, task_frames_map[tj], device)
       row[j] = acc
   return row




def compute_cl_metrics_partial(R_rows, R_star, upto_i):
   """
   R_rows: [R[0,:], R[1,:], ..., R[i,:]] stacked (list of 1D arrays)
   upto_i: í˜„ì¬ ë‹¨ê³„ i (ì‘ì—… iê¹Œì§€ í•™ìŠµ ì™„ë£Œ í›„)  # i in [1..T]
   R*: ë‹¨ì¼ì‘ì—… ìµœê³  ì„±ëŠ¥ ë¦¬ìŠ¤íŠ¸ (ê¸¸ì´ T)


   ì •ì˜(Lopez-Paz & Ranzato 2017):
     BWT = mean_{j<i} (R[i,j] - R[j,j])
     FWT = mean_{j<=i, j>0} (R[j-1, j] - R[0, j])
     IM  = mean_{j<=i, j>0} (R*_j - R[j,j])
   """
   R = np.vstack(R_rows[: upto_i + 1])  # (upto_i+1, T)
   T = R.shape[1]
   i = upto_i


   # BWT: ê³¼ê±° ì‘ì—…ì— ëŒ€í•œ í˜„ì¬(ií–‰) ì„±ëŠ¥ ë³€í™”
   bwt_terms = [R[i, j] - R[j, j] for j in range(0, min(i, T))]
   BWT = float(np.mean(bwt_terms)) if bwt_terms else 0.0


   # FWT: ì‘ì—… jë¥¼ ë°°ìš°ê¸° ì§ì „ ì„±ëŠ¥ì˜ í–¥ìƒ (R[j-1, j] - R[0, j])
   fwt_terms = [R[j - 1, j] - R[0, j] for j in range(1, min(i, T - 1) + 1)]
   FWT = float(np.mean(fwt_terms)) if fwt_terms else 0.0


   # IM: ë‹¨ì¼í•™ìŠµ ê¸°ì¤€ ëŒ€ë¹„ ë¹„í•™ìŠµì„±
   im_terms = [R_star[j] - R[j, j] for j in range(1, min(i, T - 1) + 1)]
   IM = float(np.mean(im_terms)) if im_terms else 0.0


   return {"BWT": BWT, "FWT": FWT, "IM": IM}




# -----------------------------
# ë‹¨ì¼ ì‘ì—… ê¸°ì¤€ ì„±ëŠ¥ (R*) ê³„ì‚°
# -----------------------------
def single_task_best_performance(
   agent_factory, task_name, train_datasets, task_frames_map, device, epochs=20
):
   agent = agent_factory()
   s, a, r, ns, d = train_datasets[task_name]
   agent.train_offline(s, a, r, ns, d, epochs=epochs, beta=5, keep_buffer=False)
   acc, _ = evaluate_on_frames(agent, task_frames_map[task_name], device)
   return acc




# ==================================
# ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==================================
start_time = time.time()


# ëœë¤ ì‹œë“œ ì„¤ì • (ì‹œê°„ ê¸°ë°˜)
set_seed_with_time()


DATA_FILE_T1 = "task1_data.npz"
DATA_FILE_T6 = "task6_data.npz"
DATA_FILE_T2 = "task2_data.npz"
DATA_FILE_T3 = "task3_data.npz"


# â—ï¸ ê²½ë¡œë¥¼ ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”
video_files_t1 = [
   "/home/jieun/test/1_1.mov",
   "/home/jieun/test/1_2.mov",
   "/home/jieun/test/1_3.mov",
   "/home/jieun/test/1_4.mov",
]
video_files_t6 = [
   "/home/jieun/test/6_1.mov",
   "/home/jieun/test/6_2.mov",
   "/home/jieun/test/6_3.mov",
   "/home/jieun/test/6_4.mov",
]
video_files_t2 = [
   "/home/jieun/test/2_1.mov",
   "/home/jieun/test/2_2.mov",
   "/home/jieun/test/2_3.mov",
   "/home/jieun/test/2_4.mov",
]
video_files_t3 = [
   "/home/jieun/test/3_1.mov",
   "/home/jieun/test/3_2.mov",
   "/home/jieun/test/3_3.mov",
   "/home/jieun/test/3_4.mov",
]
eval_video_file = "/home/jieun/test/test.mov"




# --- ë°ì´í„° ì¤€ë¹„ (ìºì‹œ ì‚¬ìš©) ---
def load_or_create(npz_path, video_files):
   if os.path.exists(npz_path):
       print(f"Loading data from '{npz_path}'")
       data = np.load(npz_path, allow_pickle=True)
       return data["s"], data["a"], data["r"], data["ns"], data["d"]
   else:
       print(f"Creating and saving data to '{npz_path}'")
       s, a, r, ns, d = load_and_process_data(video_files)
       np.savez(npz_path, s=s, a=a, r=r, ns=ns, d=d)
       return s, a, r, ns, d




s1, a1, r1, ns1, d1 = load_or_create(DATA_FILE_T1, video_files_t1)
s6, a6, r6, ns6, d6 = load_or_create(DATA_FILE_T6, video_files_t6)
s2, a2, r2, ns2, d2 = load_or_create(DATA_FILE_T2, video_files_t2)
s3, a3, r3, ns3, d3 = load_or_create(DATA_FILE_T3, video_files_t3)


data_prep_time = time.time() - start_time
print(f"Data preparation finished. (Took {data_prep_time:.2f} seconds)")


# --- ì‘ì—… ì •ì˜ ---
train_order = ["task1", "task6", "task2", "task3"]
train_datasets = {
   "task1": (s1, a1, r1, ns1, d1),
   "task6": (s6, a6, r6, ns6, d6),
   "task2": (s2, a2, r2, ns2, d2),
   "task3": (s3, a3, r3, ns3, d3),
}
task_frames_map = {
   "task1": frames_for_task(video_files_t1, max_frames=None),
   "task6": frames_for_task(video_files_t6, max_frames=None),
   "task2": frames_for_task(video_files_t2, max_frames=None),
   "task3": frames_for_task(video_files_t3, max_frames=None),
}




def agent_factory():
   return DQNAgent(
       state_dim=3,
       action_dim=2,
       device=device,
       cql_alpha=30,
       lr=1e-5,
       gamma=0.99,
       batch_size=32,
       target_update_freq=10,
       ewc_lambda=10,  # EWC ê°•ë„ ì¡°ì ˆ ê°€ëŠ¥
   )




# ----- R* í•œ ë²ˆë§Œ ë¯¸ë¦¬ ê³„ì‚°(ì‹¤í—˜ ë°˜ë³µ ì „ì—) -----
R_star = []
for tj in train_order:
   acc_star = single_task_best_performance(
       agent_factory=agent_factory,
       task_name=tj,
       train_datasets=train_datasets,
       task_frames_map=task_frames_map,
       device=device,
       epochs=20,
   )
   R_star.append(acc_star)
print("\n[R* ready] single-task best accuracies:", [f"{v:.5f}" for v in R_star])


# --- ë°˜ë³µ ì‹¤í—˜ ---
# âš™ï¸ ì„¤ì •: False = í•œ ë²ˆë§Œ ì‹¤í–‰ (ë¹ ë¦„), True = 3íšŒ ë°˜ë³µ (ëŠë¦¬ì§€ë§Œ ì •í™•)
USE_MULTIPLE_RUNS = False  # Trueë¡œ ë°”ê¾¸ë©´ 3íšŒ ì‹¤í–‰
num_runs = 3 if USE_MULTIPLE_RUNS else 1
all_accuracies = []  # test.mov accuracy ê¸°ë¡


for run_idx in range(num_runs):
   print(f"\n{'='*20} Experiment {run_idx+1}/{num_runs} Start {'='*20}")
   agent = agent_factory()


   # ---- (A) ë¯¸í•™ìŠµ ì„±ëŠ¥ R[0,:] í™•ë³´ ----
   R_rows = []
   R0 = evaluate_all_tasks(agent, train_order, task_frames_map, device)
   R_rows.append(R0)
   print(f"[Stage 0] Untrained Acc per task: {[f'{v:.5f}' for v in R0]}")


   # ---- (B) ìˆœì°¨ í•™ìŠµ + ê° ì‘ì—… ì¢…ë£Œ ì‹œ ì§€í‘œ ê³„ì‚°/ì¶œë ¥ ----


   # Task 1
   print("\n--- Training on Task 1 ---")
   s, a, r, ns, d = train_datasets["task1"]
   agent.train_offline(s, a, r, ns, d, epochs=20, beta=5, keep_buffer=False)
   agent.save_params_and_fisher(list(zip(s, a, r, ns, d)), "task1")
   R1 = evaluate_all_tasks(agent, train_order, task_frames_map, device)
   R_rows.append(R1)
   m1 = compute_cl_metrics_partial(R_rows, R_star, upto_i=1)
   print(
       f"[After Task 1] Acc per task: {[f'{v:.5f}' for v in R1]} | FWT={m1['FWT']:.5f}, BWT={m1['BWT']:.5f}, IM={m1['IM']:.5f}"
   )
   print(f"  ğŸ“Š Task ì •í™•ë„ ë³€í™”: {[f'{x:.5f}' for x in R0]} â†’ {[f'{x:.5f}' for x in R1]}")


   # Task 6 (ì´ì „ ë°ì´í„° ìœ ì§€!)
   print("\n--- Training on Task 6 (Continual) ---")
   s, a, r, ns, d = train_datasets["task6"]
   agent.train_offline(s, a, r, ns, d, epochs=20, beta=5, keep_buffer=True)
   agent.save_params_and_fisher(list(zip(s, a, r, ns, d)), "task6")
   R2 = evaluate_all_tasks(agent, train_order, task_frames_map, device)
   R_rows.append(R2)
   m2 = compute_cl_metrics_partial(R_rows, R_star, upto_i=2)
   print(
       f"[After Task 6] Acc per task: {[f'{v:.5f}' for v in R2]} | FWT={m2['FWT']:.5f}, BWT={m2['BWT']:.5f}, IM={m2['IM']:.5f}"
   )


   # Task 2 (ì´ì „ ë°ì´í„° ìœ ì§€!)
   print("\n--- Training on Task 2 (Continual) ---")
   s, a, r, ns, d = train_datasets["task2"]
   agent.train_offline(s, a, r, ns, d, epochs=20, beta=5, keep_buffer=True)
   agent.save_params_and_fisher(list(zip(s, a, r, ns, d)), "task2")
   R3 = evaluate_all_tasks(agent, train_order, task_frames_map, device)
   R_rows.append(R3)
   m3 = compute_cl_metrics_partial(R_rows, R_star, upto_i=3)
   print(
       f"[After Task 2] Acc per task: {[f'{v:.5f}' for v in R3]} | FWT={m3['FWT']:.5f}, BWT={m3['BWT']:.5f}, IM={m3['IM']:.5f}"
   )


   # Task 3 (ì´ì „ ë°ì´í„° ìœ ì§€!)
   print("\n--- Training on Task 3 (Continual) ---")
   s, a, r, ns, d = train_datasets["task3"]
   agent.train_offline(s, a, r, ns, d, epochs=20, beta=5, keep_buffer=True)
   agent.save_params_and_fisher(list(zip(s, a, r, ns, d)), "task3")
   R4 = evaluate_all_tasks(agent, train_order, task_frames_map, device)
   R_rows.append(R4)
   m4 = compute_cl_metrics_partial(R_rows, R_star, upto_i=4)
   print(
       f"[After Task 3] Acc per task: {[f'{v:.5f}' for v in R4]} | FWT={m4['FWT']:.5f}, BWT={m4['BWT']:.5f}, IM={m4['IM']:.5f}"
   )
   print(f"  ğŸ“Š ìµœì¢… Task ì •í™•ë„: {[f'{x:.5f}' for x in R4]}")
   print(f"  ğŸ”„ ë¬´í•™ìŠµ ëŒ€ë¹„ ê°œì„ : {[f'{x:.5f}' for x in R0]} â†’ {[f'{x:.5f}' for x in R4]}")


   # ---- (C) test.mov: Accuracyë§Œ ----
   acc_only, _ = evaluate_agent(agent, eval_video_file, run_index=run_idx)
   print(
       f"=== [Run {run_idx+1}] test.mov Accuracy: {acc_only:.5f} ({acc_only*100:.5f}%) ==="
   )
   all_accuracies.append(acc_only)


# --- ìµœì¢… ê²°ê³¼ ìš”ì•½(Accuracyë§Œ) ---
total_run_time = time.time() - start_time
print(f"\n\n{'='*20} Final Summary of {num_runs} Experiments {'='*20}")
print(f"Total time: {total_run_time:.5f} seconds")
if all_accuracies:
   mean_accuracy = np.mean(all_accuracies)
   std_accuracy = np.std(all_accuracies)
   print(
       f"Average test.mov Accuracy: {mean_accuracy:.5f} (Std Dev: {std_accuracy:.5f})"
   )
   print("\nIndividual Accuracies:", [f"{acc:.5f}" for acc in all_accuracies])









